{"cells":[{"cell_type":"markdown","source":["Tip: I used Google Colab and I mounted my drive (see below)\n","To run it, please ensure you import the same structure as in the 2nd cell, and not only the notebook"],"metadata":{"id":"evQFR5_p2jTe"}},{"cell_type":"markdown","source":["I thought it would be interesting to see what could be done just from the article. The aim was to see if I could, from Hugging Face's pipeline, and the information in the article, reconstruct their work (their code)."],"metadata":{"id":"U3ymIpvZnvUi"}},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["d2ef61b17945411586f5eb4c7bb1d181","1951e0b0d3fb4a4083ca0706211173f3","299fc7022ff040738a73574ad041bafc","2fbf30c7a5c242ea898e6f50da708c8d","cb6bd955c4e3463fa48642b4e54b3f97","7fceeaf9da5442a896c8c8b4cff232e6","30367fe1ae08426a882757cb338ba5ac","e3951641f6684bcda81d9fa4d252878e","a4057474485e40b5b42b998cff104ac2","2737301103594c318be6df90e007617f","32a6774fa0264d9c8457ddc2d8a814b3"]},"id":"w6ILYc3yEFgm","executionInfo":{"status":"ok","timestamp":1736156280042,"user_tz":-60,"elapsed":1842,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"06aa7fc9-f53d-4a54-fab7-48ab66861867"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ef61b17945411586f5eb4c7bb1d181"}},"metadata":{}}],"source":["# pipeline from Hugging Face\n","import torch\n","from diffusers import StableDiffusionPipeline\n","import torch\n","from PIL import Image\n","import numpy as np\n","\n","model_id = \"mlpc-lab/TokenCompose_SD14_A\"\n","\n","pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1681,"status":"ok","timestamp":1736156281713,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"},"user_tz":-60},"id":"0xPJQbP0QhAI","outputId":"1749da2c-f1b9-435a-e7e0-0b24a34f1300"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab/Projet\n","/content/drive/MyDrive/Colab/Projet\n","coco_gsam_seg  src.ipynb\n"]}],"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab/Projet\n","!pwd  # check that you're in the right directory\n","!ls # you should see coco_gsam_seg and src.ipynb"]},{"cell_type":"markdown","metadata":{"id":"g-Tc7-JQZp4n"},"source":["### CPU (plantage RAM assez rapide)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":127},"id":"EeB4sXjPEFgo","outputId":"5ed9e677-4c4b-4b52-95ca-5f79b7313bdd","executionInfo":{"status":"ok","timestamp":1736156281714,"user_tz":-60,"elapsed":9,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndata = [\\n    {\\n        \"file_name\": \"000000000142.jpg\",\\n        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\\n        \"attn_list\": [\\n            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\\n            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\\n            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\\n            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\\n            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\\n            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\\n            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\\n        ]\\n    },\\n    {\\n        \"file_name\": \"000000000370.jpg\",\\n        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\\n        \"attn_list\": [\\n            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\\n            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\\n        ]\\n    }\\n]\\n\\ntok=pipe.components[\\'tokenizer\\']\\nenc=pipe.components[\\'text_encoder\\']\\nvae=pipe.components[\\'vae\\']\\nune=pipe.components[\\'unet\\']\\n\\nfor d in data:\\n  # récupération du prompt et passage par le tokenizer puis text_encoder\\n  toks=tok(text=d[\"text\"],return_tensors=\\'pt\\')\\n  encs=enc(input_ids=toks[\\'input_ids\\'],attention_mask=toks[\\'attention_mask\\'])\\n  print(encs.last_hidden_state.shape)\\n\\n  # récupération de l\\'image et passage dans le vae\\n  photo_path=\"./coco_gsam_seg/\"+d[\"file_name\"][0:-4]+\"/src.jpg\"\\n  image=Image.open(photo_path)\\n  image=image.resize((512, 512))\\n  image_array=np.array(image)\\n  image_tensor=torch.tensor(image_array, dtype=torch.float32).permute(2,0,1)/255.0\\n  print(image_tensor.shape)\\n  lat=vae.encode(image_tensor.unsqueeze(0))\\n  lat_tens=lat.latent_dist.sample()\\n  print(lat_tens.shape)\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["'''\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder']\n","vae=pipe.components['vae']\n","une=pipe.components['unet']\n","\n","for d in data:\n","  # récupération du prompt et passage par le tokenizer puis text_encoder\n","  toks=tok(text=d[\"text\"],return_tensors='pt')\n","  encs=enc(input_ids=toks['input_ids'],attention_mask=toks['attention_mask'])\n","  print(encs.last_hidden_state.shape)\n","\n","  # récupération de l'image et passage dans le vae\n","  photo_path=\"./coco_gsam_seg/\"+d[\"file_name\"][0:-4]+\"/src.jpg\"\n","  image=Image.open(photo_path)\n","  image=image.resize((512, 512))\n","  image_array=np.array(image)\n","  image_tensor=torch.tensor(image_array, dtype=torch.float32).permute(2,0,1)/255.0\n","  print(image_tensor.shape)\n","  lat=vae.encode(image_tensor.unsqueeze(0))\n","  lat_tens=lat.latent_dist.sample()\n","  print(lat_tens.shape)\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"oSVV63h1ZoS-"},"source":["### GPU (small test with 2 dataset images and batch size=1)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1538,"status":"ok","timestamp":1736156283245,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"},"user_tz":-60},"id":"pYk_yl7PS0qZ","outputId":"2946aa9e-4db7-4822-fb40-a4230f5db9af"},"outputs":[{"output_type":"stream","name":"stdout","text":["using GPU\n","torch.Size([1, 20, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 16.387744903564453\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 9.494438171386719\n","tensor(25.8822, device='cuda:0', grad_fn=<AddBackward0>)\n","torch.Size([1, 14, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 23.554521560668945\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 1.4836835861206055\n","tensor(25.0382, device='cuda:0', grad_fn=<AddBackward0>)\n"]}],"source":["if torch.cuda.is_available():\n","  device=torch.device(\"cuda\")\n","  print(f\"using GPU\")\n","else:\n","  device=torch.device(\"cpu\")\n","  print(f\"using CPU\")\n","\n","# for the moment we are only considering the first two lines of the dataset\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","# load components\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder'].to(device)\n","vae=pipe.components['vae'].to(device)\n","une=pipe.components['unet'].to(device)\n","sch=pipe.components['scheduler']\n","\n","for d in data:\n","    # Retrieve the prompt and run it through the tokenizer (CPU) then text_encoder (GPU)\n","    toks=tok(text=d[\"text\"],return_tensors='pt') # if I don't fill in return_tensors, the elements of toks will be lists (but encs takes tensors as inputs) -> 'pt'=\"pytorch\"\n","    toks={Key:Value.to(device) for Key,Value in toks.items()}  # transfer the data on GPU\n","    encs=enc(input_ids=toks['input_ids'],attention_mask=toks['attention_mask'])\n","    print(encs.last_hidden_state.shape)\n","    # torch.Size([1, 20, 768]) for the 1st image\n","    # torch.Size([1, 14, 768]) for the 2nd image (only the length of sequence varies)\n","    # (batch size, sequence length, dimension of text embedding)\n","\n","    # Retrieving the image and transferring it to the vae\n","    photo_path = \"./coco_gsam_seg/\" + d[\"file_name\"][0:-4] + \"/src.jpg\"\n","    photo=Image.open(photo_path)\n","    photo=photo.resize((512, 512))\n","    photo_array=np.array(photo)\n","    photo_tensor=torch.tensor(photo_array, dtype=torch.float32).permute(2,0,1)/255.0\n","    photo_tensor=photo_tensor.unsqueeze(0).to(device)  # vae takes 4-dimension tensor as input so we must add a first dimension artificially\n","    print(photo_tensor.shape)\n","    # torch.Size([1, 3, 512, 512])\n","\n","    # encode the photo with autoencoder kl to obtain latent\n","    lat=vae.encode(photo_tensor)\n","    lat_tens=lat.latent_dist.sample() # obtain the tensor associated with the latent\n","    print(lat_tens.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # add noise\n","    # here I used the code from https://github.com/mlpc-ucsd/TokenCompose\n","    noise = torch.randn_like(lat_tens)\n","    bsz = lat_tens.shape[0]\n","    max_timestep = sch.config.num_train_timesteps\n","    timesteps = torch.randint(0, max_timestep, (bsz,), device=lat_tens.device)\n","    timesteps = timesteps.long()\n","    noisy_latents = sch.add_noise(lat_tens, noise, timesteps)\n","    print(noisy_latents.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # use unet\n","    prediction=une(noisy_latents,timesteps,encs.last_hidden_state).sample\n","    print(prediction.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # Loss LDM\n","    loss_ldm=torch.nn.functional.mse_loss(prediction,lat_tens)\n","    print(f\"loss_ldm: {loss_ldm}\")\n","    # tensor(16.7185, device='cuda:0', grad_fn=<MseLossBackward0>) for the 1st image\n","    # tensor(19.2932, device='cuda:0', grad_fn=<MseLossBackward0>) fir the 2nd image\n","\n","    # Ltoken\n","    ltoken=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      pixelwise_product=photo_tensor_token*prediction  # normally we use the attention activations but for the moment I don't have them (we look at that later and use \"prediction\" instead for the moment)\n","      sum_pixelwise=pixelwise_product.sum()\n","      sum_prediction=prediction.sum()\n","      ratio=sum_pixelwise/sum_prediction\n","      result=(1-ratio)**2\n","      ltoken+=result\n","    print(f\"ltoken: {ltoken}\")\n","\n","    # Lpixel\n","    # disabled because not working properly\n","    '''\n","    lpixel=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      log_prediction=torch.log(prediction)  # log(prediction)\n","      log_one_minus_prediction=torch.log(1-prediction)  # log(1-prediction)\n","      a=image_tensor_token*log_prediction  # image_tensor_token[u]*log(prediction[u])\n","      b=(1-image_tensor_token)*log_one_minus_prediction  # (1-image_tensor_token[u])*log(1-prediction[u])\n","      final_result=(a+b).sum()\n","      lpixel+=final_result\n","    print(f\"lpixel: {lpixel}\")\n","    '''\n","\n","    loss=loss_ldm+ltoken\n","    print(loss)\n"]},{"cell_type":"markdown","source":["### GPU (small test with 2 dataset images and batch size=2)\n","Big difference with before: remember to add `padding=True` in the tokenizer"],"metadata":{"id":"hygyJSwqwb3F"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device=torch.device(\"cuda\")\n","  print(f\"using GPU\")\n","else:\n","  device=torch.device(\"cpu\")\n","  print(f\"using CPU\")\n","\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","# load components\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder'].to(device)\n","vae=pipe.components['vae'].to(device)\n","une=pipe.components['unet'].to(device)\n","sch=pipe.components['scheduler']\n","\n","# create lists\n","photos=[]\n","texts=[]\n","attn_lists=[]\n","\n","# collect data\n","for d in data:\n","    texts.append(d[\"text\"])\n","    attn_lists.append(d[\"attn_list\"])\n","    photo_path=\"./coco_gsam_seg/\" + d[\"file_name\"][0:-4] + \"/src.jpg\"\n","    photo=Image.open(photo_path)\n","    photo=photo.resize((512, 512))\n","    photo_array=np.array(photo)\n","    photo_tensor=torch.tensor(photo_array, dtype=torch.float32).permute(2,0,1)/255.0\n","    photos.append(photo_tensor)\n","\n","# convert lists to tensors\n","texts_batch=tok(text=texts, return_tensors='pt',padding=True) # wde must activate padding here\n","texts_batch={Key: Value.to(device) for Key,Value in texts_batch.items()}\n","photos_batch=torch.stack([photo.to(device) for photo in photos])\n","\n","# go through encoder\n","encs=enc(input_ids=texts_batch['input_ids'], attention_mask=texts_batch['attention_mask'])\n","\n","# go through vae\n","latents=vae.encode(photos_batch)\n","lat_tens=latents.latent_dist.sample() # recover specifically the tensor\n","\n","# add noise\n","# here I used the code from https://github.com/mlpc-ucsd/TokenCompose\n","noise = torch.randn_like(lat_tens)\n","bsz = lat_tens.shape[0]  # Taille du batch\n","max_timestep=sch.config.num_train_timesteps\n","timesteps = torch.randint(0, max_timestep, (bsz,), device=lat_tens.device)\n","timesteps = timesteps.long()\n","noisy_latents=sch.add_noise(lat_tens, noise, timesteps)\n","\n","# use unet\n","predictions=une(noisy_latents,timesteps,encs.last_hidden_state).sample\n","\n","# Loss LDM\n","loss_ldm=torch.nn.functional.mse_loss(predictions,lat_tens)\n","\n","# Ltoken and Lpixel\n","ltoken=0.0\n","for k, d in enumerate(data):\n","    for token in d[\"attn_list\"]:\n","        photo_path_token=\"./coco_gsam_\"+token[1]\n","        photo_token=Image.open(photo_path_token)\n","        photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","        photo_array_token=np.array(photo_token)\n","        photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","\n","        photo_tensor_token=photo_tensor_token.to(device)\n","        photo_tensor_token=photo_tensor_token.unsqueeze(0)\n","\n","        # pixekwise product\n","        pixelwise_product=photo_tensor_token*predictions[k:k+1]  # instead of using the cross attention map I use predictions for the moment because I don't know how to get the cross attention map\n","        sum_pixelwise=pixelwise_product.sum()\n","        sum_prediction=predictions[k:k+1].sum()\n","        ratio=sum_pixelwise/sum_prediction\n","        result=(1-ratio)**2\n","        ltoken+=result\n","\n","# total loss\n","loss=loss_ldm +ltoken\n","print(loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rS95XeFowBl2","executionInfo":{"status":"ok","timestamp":1736156283600,"user_tz":-60,"elapsed":358,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"33dfc9e4-a4de-4f40-f53b-f56bce95c5e3"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["using GPU\n","tensor(31.3645, device='cuda:0', grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","source":["### Where is the cross attention map ?"],"metadata":{"id":"Zsq9QRFmx53g"}},{"cell_type":"code","source":["une.attn_processors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPqIf1dd5fqY","executionInfo":{"status":"ok","timestamp":1736156283600,"user_tz":-60,"elapsed":4,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"18eac7bb-c766-4578-ab13-eb10be74d53b"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a54a90>,\n"," 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a56e60>,\n"," 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a544f0>,\n"," 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a56e30>,\n"," 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e89110c0>,\n"," 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8912800>,\n"," 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8911cc0>,\n"," 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8911720>,\n"," 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e89101f0>,\n"," 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8912200>,\n"," 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e89128c0>,\n"," 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8911ea0>,\n"," 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e893caf0>,\n"," 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e893d210>,\n"," 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e893cd00>,\n"," 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e893c0d0>,\n"," 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e893c970>,\n"," 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e893c550>,\n"," 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f76d0>,\n"," 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f7670>,\n"," 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f6980>,\n"," 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f70a0>,\n"," 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f4910>,\n"," 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f4b20>,\n"," 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f6d40>,\n"," 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f7d00>,\n"," 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f70d0>,\n"," 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f4df0>,\n"," 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f55d0>,\n"," 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e85f4790>,\n"," 'mid_block.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8913250>,\n"," 'mid_block.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8910af0>}"]},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["### Try recovering attention map with a hook\n","\n","Using the previous code GPU (small test with 2 dataset images and batch size=1)"],"metadata":{"id":"HviiRvDvzqjp"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device=torch.device(\"cuda\")\n","  print(f\"using GPU\")\n","else:\n","  device=torch.device(\"cpu\")\n","  print(f\"using CPU\")\n","\n","# for the moment we are only considering the first two lines of the dataset\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","# load components\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder'].to(device)\n","vae=pipe.components['vae'].to(device)\n","une=pipe.components['unet'].to(device)\n","sch=pipe.components['scheduler']\n","\n","for d in data:\n","    # Retrieve the prompt and run it through the tokenizer (CPU) then text_encoder (GPU)\n","    toks=tok(text=d[\"text\"],return_tensors='pt') # if I don't fill in return_tensors, the elements of toks will be lists (but encs takes tensors as inputs) -> 'pt'=\"pytorch\"\n","    toks={Key:Value.to(device) for Key,Value in toks.items()}  # transfer the data on GPU\n","    encs=enc(input_ids=toks['input_ids'],attention_mask=toks['attention_mask'])\n","    print(encs.last_hidden_state.shape)\n","    # torch.Size([1, 20, 768]) for the 1st image\n","    # torch.Size([1, 14, 768]) for the 2nd image (only the length of sequence varies)\n","    # (batch size, sequence length, dimension of text embedding)\n","\n","    # Retrieving the image and transferring it to the vae\n","    photo_path = \"./coco_gsam_seg/\" + d[\"file_name\"][0:-4] + \"/src.jpg\"\n","    photo=Image.open(photo_path)\n","    photo=photo.resize((512, 512))\n","    photo_array=np.array(photo)\n","    photo_tensor=torch.tensor(photo_array, dtype=torch.float32).permute(2,0,1)/255.0\n","    photo_tensor=photo_tensor.unsqueeze(0).to(device)  # vae takes 4-dimension tensor as input so we must add a first dimension artificially\n","    print(photo_tensor.shape)\n","    # torch.Size([1, 3, 512, 512])\n","\n","    # encode the photo with autoencoder kl to obtain latent\n","    lat=vae.encode(photo_tensor)\n","    lat_tens=lat.latent_dist.sample() # obtain the tensor associated with the latent\n","    print(lat_tens.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # add noise\n","    # here I used the code from https://github.com/mlpc-ucsd/TokenCompose\n","    noise = torch.randn_like(lat_tens)\n","    bsz = lat_tens.shape[0]\n","    max_timestep = sch.config.num_train_timesteps\n","    timesteps = torch.randint(0, max_timestep, (bsz,), device=lat_tens.device)\n","    timesteps = timesteps.long()\n","    noisy_latents = sch.add_noise(lat_tens, noise, timesteps)\n","    print(noisy_latents.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    '''\n","    We add the hook just right here\n","    '''\n","    # prepare list containing activations\n","    activations=[]\n","\n","    # I add a function that I use with the hook\n","    def attention_activations(module, input, output):\n","          activations.append(output)\n","\n","    transformer_block=une.down_blocks[0].attentions[0].transformer_blocks[0]  # the block from which I want to get activations\n","    transformer_block.register_forward_hook(attention_activations)\n","\n","    # use unet\n","    prediction=une(noisy_latents,timesteps,encs.last_hidden_state).sample\n","    print(prediction.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # Loss LDM\n","    loss_ldm=torch.nn.functional.mse_loss(prediction,lat_tens)\n","    print(f\"loss_ldm: {loss_ldm}\")\n","    # tensor(16.7185, device='cuda:0', grad_fn=<MseLossBackward0>) for the 1st image\n","    # tensor(19.2932, device='cuda:0', grad_fn=<MseLossBackward0>) fir the 2nd image\n","\n","    # Ltoken\n","    ltoken=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      pixelwise_product=photo_tensor_token*prediction  # normally we use the attention activations but for the moment I don't have them (we look at that later and use \"prediction\" instead for the moment)\n","      sum_pixelwise=pixelwise_product.sum()\n","      sum_prediction=prediction.sum()\n","      ratio=sum_pixelwise/sum_prediction\n","      result=(1-ratio)**2\n","      ltoken+=result\n","    print(f\"ltoken: {ltoken}\")\n","\n","    # Lpixel\n","    # disabled because not working properly\n","    '''\n","    lpixel=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      log_prediction=torch.log(prediction)  # log(prediction)\n","      log_one_minus_prediction=torch.log(1-prediction)  # log(1-prediction)\n","      a=image_tensor_token*log_prediction  # image_tensor_token[u]*log(prediction[u])\n","      b=(1-image_tensor_token)*log_one_minus_prediction  # (1-image_tensor_token[u])*log(1-prediction[u])\n","      final_result=(a+b).sum()\n","      lpixel+=final_result\n","    print(f\"lpixel: {lpixel}\")\n","    '''\n","\n","    loss=loss_ldm+ltoken\n","    print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8E22Yl9W59rE","executionInfo":{"status":"ok","timestamp":1736156283918,"user_tz":-60,"elapsed":320,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"bd49a212-96c3-43bb-aaf3-7b55a365d2bc"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["using GPU\n","torch.Size([1, 20, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 16.33377456665039\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 10.115011215209961\n","tensor(26.4488, device='cuda:0', grad_fn=<AddBackward0>)\n","torch.Size([1, 14, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 21.954076766967773\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 1.3780158758163452\n","tensor(23.3321, device='cuda:0', grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["print(activations[1].shape) # renvoie [1,4096,320]. Or 4096 correspond à 64x64, clairement le nombre de cases de lat_tens\n","# Mais à quoi correspond 320 ?"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNBddTws73i1","executionInfo":{"status":"ok","timestamp":1736156283919,"user_tz":-60,"elapsed":5,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"60096285-7484-438a-b56c-de3fc85f0be3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4096, 320])\n"]}]},{"cell_type":"markdown","source":["Ce que j'ai fait jusque là (fr, see english below):\n","* étudier en détail le code sur Hugging Face de S.D.1.4\n","* lire le papier (-> il y a une erreur sur le schéma mais globalement bon papier)\n","* tenter d'exécuter TokenCompose (-> j'ai eu un problème de bibliothèque)\n","* recoder mon propre TokenCompose grâce à ma connaissance du papier et des modules d'Hugging Face. Je me suis dit que ce serait finalement le meilleur moyen de comprendre ce qui se passe dans TokenCompose et unet. Je n'ai pas bien compris comment récupérer la carte des activations (dans les attentions layers) car dimension [1,4096,320] (j'ai bien compris que 4096 c'est pour 64x64 car l'espace latent est 64x64 mais je n'ai pas compris le rapport entre 320 et les dimensions de l'embedding textuel.\n","\n","Mes enseignements sur TokenCompose:\n","* papier bien rédigé sauf petite erreur sur la figure 3 (les flèches sont mal reliées)\n","* j'ai pris conscience que la vraie difficulté à laquelle ils ont dû faire face est de récupérer les attentions (car je n'ai pas réussi pour l'instant)... mais j'ai bien compris que théoriquement avec un `hook` ça doit être possible\n","\n","Enseignements généraux:\n","* j'ai appris à manipuler Hugging Face (la plateforme)\n","* J'ai réalisé qu'Hugging Face est faite pour que les modèles soient utilisés, mais pas forcément fine tunés. Par exemple, SD14 n'a pas de fonction forward. il n'a qu'une fonction __call__\n","* on peut facilement accéder au code des modèles sur Hugging Face mais difficile à lire car beaucoup de classes imbriquées les unes dans les autres"],"metadata":{"id":"xph2FrcRBKJJ"}},{"cell_type":"markdown","source":["What I have done so far:\n","* study the Hugging Face code from S.D.1.4 in detail\n","* read the paper (-> there's an error in the schema but overall a good paper)\n","* try to run TokenCompose (-> I had a library problem)\n","* recode my own TokenCompose using my knowledge of the paper and Hugging Face modules. I figured this would ultimately be the best way to understand what was going on in TokenCompose and unet. I didn't quite understand how to get the activation map (in the attentions layers) because dimension [1,4096,320] (I understood that 4096 is for 64x64 because the latent space is 64x64 but I didn't understand the relationship between 320 and the dimensions of the text embedding.\n","\n","What I learned about TokenCompose:\n","* well-written paper except for a small error on figure 3 (the arrows are not connected correctly).\n","* I've realised that the real difficulty they've had to face is getting the attentions back (because I haven't managed that yet)... but I've understood that theoretically with a `hook` it should be possible.\n","\n","General lessons:\n","\n","* I learnt how to handle Hugging Face (the platform)\n","* I realised that Hugging Face is designed for models to be used, but not necessarily fine-tuned. For example, SD14 has no forward function, only a __call__ function.\n","* You can easily access the template code on Hugging Face, but it's difficult to read because there are so many classes nested inside each other."],"metadata":{"id":"_4MxEIMP16jX"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d2ef61b17945411586f5eb4c7bb1d181":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1951e0b0d3fb4a4083ca0706211173f3","IPY_MODEL_299fc7022ff040738a73574ad041bafc","IPY_MODEL_2fbf30c7a5c242ea898e6f50da708c8d"],"layout":"IPY_MODEL_cb6bd955c4e3463fa48642b4e54b3f97"}},"1951e0b0d3fb4a4083ca0706211173f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fceeaf9da5442a896c8c8b4cff232e6","placeholder":"​","style":"IPY_MODEL_30367fe1ae08426a882757cb338ba5ac","value":"Loading pipeline components...: 100%"}},"299fc7022ff040738a73574ad041bafc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3951641f6684bcda81d9fa4d252878e","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4057474485e40b5b42b998cff104ac2","value":7}},"2fbf30c7a5c242ea898e6f50da708c8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2737301103594c318be6df90e007617f","placeholder":"​","style":"IPY_MODEL_32a6774fa0264d9c8457ddc2d8a814b3","value":" 7/7 [00:01&lt;00:00,  4.86it/s]"}},"cb6bd955c4e3463fa48642b4e54b3f97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fceeaf9da5442a896c8c8b4cff232e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30367fe1ae08426a882757cb338ba5ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3951641f6684bcda81d9fa4d252878e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4057474485e40b5b42b998cff104ac2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2737301103594c318be6df90e007617f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32a6774fa0264d9c8457ddc2d8a814b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}