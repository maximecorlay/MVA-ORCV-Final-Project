{"cells":[{"cell_type":"markdown","source":["Tip: I used Google Colab and I mounted my drive (see below)\n","To run it, please ensure you import the same structure as in the 2nd cell, and not only the notebook"],"metadata":{"id":"evQFR5_p2jTe"}},{"cell_type":"markdown","source":["I thought it would be interesting to see what could be done just from the article. The aim was to see if I could, from Hugging Face's pipeline, and the information in the article, reconstruct their work (their code)."],"metadata":{"id":"U3ymIpvZnvUi"}},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["63ac5207aaf84ad09c4a46f59d82db0f","04f9d45d296a4777b13906a9d07602ab","a5f2c49ec52e48a8afddffa5f2acd93d","08e5a5039da941b391350c469cdf8df3","a3c1798c707f4df086e8afdc47eaeaeb","54536a2a79f74c8981b07114c710ce10","7f0c9207d4e9470292d0574b88df8ac7","1d5be3fb464d4f9c9e9343c1403a59a4","4e2d2bca2d2a49f7afa7133b3bd0291c","69790ca66b574a4f804b507ae5088169","8661fd0d7ac94b8498c4428694821c54"]},"id":"w6ILYc3yEFgm","executionInfo":{"status":"ok","timestamp":1736154058647,"user_tz":-60,"elapsed":1753,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"d740644f-bfa3-4bd3-e1c6-e8c95746a2a2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ac5207aaf84ad09c4a46f59d82db0f"}},"metadata":{}}],"source":["# pipeline from Hugging Face\n","import torch\n","from diffusers import StableDiffusionPipeline\n","import torch\n","from PIL import Image\n","import numpy as np\n","\n","model_id = \"mlpc-lab/TokenCompose_SD14_A\"\n","\n","pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1554,"status":"ok","timestamp":1736154060198,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"},"user_tz":-60},"id":"0xPJQbP0QhAI","outputId":"5d46f280-5551-400d-dd48-1065434cbe67"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab/Projet\n","/content/drive/MyDrive/Colab/Projet\n","coco_gsam_seg  src.ipynb\n"]}],"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/Colab/Projet\n","!pwd  # check that you're in the right directory\n","!ls # you should see coco_gsam_seg and src.ipynb"]},{"cell_type":"markdown","metadata":{"id":"g-Tc7-JQZp4n"},"source":["### CPU (plantage RAM assez rapide)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":127},"id":"EeB4sXjPEFgo","outputId":"90a1279c-5a14-48fe-b88a-9a1a567cff2c","executionInfo":{"status":"ok","timestamp":1736154060198,"user_tz":-60,"elapsed":5,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndata = [\\n    {\\n        \"file_name\": \"000000000142.jpg\",\\n        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\\n        \"attn_list\": [\\n            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\\n            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\\n            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\\n            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\\n            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\\n            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\\n            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\\n        ]\\n    },\\n    {\\n        \"file_name\": \"000000000370.jpg\",\\n        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\\n        \"attn_list\": [\\n            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\\n            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\\n        ]\\n    }\\n]\\n\\ntok=pipe.components[\\'tokenizer\\']\\nenc=pipe.components[\\'text_encoder\\']\\nvae=pipe.components[\\'vae\\']\\nune=pipe.components[\\'unet\\']\\n\\nfor d in data:\\n  # récupération du prompt et passage par le tokenizer puis text_encoder\\n  toks=tok(text=d[\"text\"],return_tensors=\\'pt\\')\\n  encs=enc(input_ids=toks[\\'input_ids\\'],attention_mask=toks[\\'attention_mask\\'])\\n  print(encs.last_hidden_state.shape)\\n\\n  # récupération de l\\'image et passage dans le vae\\n  photo_path=\"./coco_gsam_seg/\"+d[\"file_name\"][0:-4]+\"/src.jpg\"\\n  image=Image.open(photo_path)\\n  image=image.resize((512, 512))\\n  image_array=np.array(image)\\n  image_tensor=torch.tensor(image_array, dtype=torch.float32).permute(2,0,1)/255.0\\n  print(image_tensor.shape)\\n  lat=vae.encode(image_tensor.unsqueeze(0))\\n  lat_tens=lat.latent_dist.sample()\\n  print(lat_tens.shape)\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["'''\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder']\n","vae=pipe.components['vae']\n","une=pipe.components['unet']\n","\n","for d in data:\n","  # récupération du prompt et passage par le tokenizer puis text_encoder\n","  toks=tok(text=d[\"text\"],return_tensors='pt')\n","  encs=enc(input_ids=toks['input_ids'],attention_mask=toks['attention_mask'])\n","  print(encs.last_hidden_state.shape)\n","\n","  # récupération de l'image et passage dans le vae\n","  photo_path=\"./coco_gsam_seg/\"+d[\"file_name\"][0:-4]+\"/src.jpg\"\n","  image=Image.open(photo_path)\n","  image=image.resize((512, 512))\n","  image_array=np.array(image)\n","  image_tensor=torch.tensor(image_array, dtype=torch.float32).permute(2,0,1)/255.0\n","  print(image_tensor.shape)\n","  lat=vae.encode(image_tensor.unsqueeze(0))\n","  lat_tens=lat.latent_dist.sample()\n","  print(lat_tens.shape)\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"oSVV63h1ZoS-"},"source":["### GPU (small test with 2 dataset images and batch size=1)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1813,"status":"ok","timestamp":1736154062007,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"},"user_tz":-60},"id":"pYk_yl7PS0qZ","outputId":"d4ebb0fe-15cf-422d-e615-a409fa84a8f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["using GPU\n","torch.Size([1, 20, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 18.226566314697266\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 8.466508865356445\n","tensor(26.6931, device='cuda:0', grad_fn=<AddBackward0>)\n","torch.Size([1, 14, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 19.947223663330078\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 1.386617660522461\n","tensor(21.3338, device='cuda:0', grad_fn=<AddBackward0>)\n"]}],"source":["if torch.cuda.is_available():\n","  device=torch.device(\"cuda\")\n","  print(f\"using GPU\")\n","else:\n","  device=torch.device(\"cpu\")\n","  print(f\"using CPU\")\n","\n","# for the moment we are only considering the first two lines of the dataset\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","# load components\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder'].to(device)\n","vae=pipe.components['vae'].to(device)\n","une=pipe.components['unet'].to(device)\n","sch=pipe.components['scheduler']\n","\n","for d in data:\n","    # Retrieve the prompt and run it through the tokenizer (CPU) then text_encoder (GPU)\n","    toks=tok(text=d[\"text\"],return_tensors='pt') # if I don't fill in return_tensors, the elements of toks will be lists (but encs takes tensors as inputs) -> 'pt'=\"pytorch\"\n","    toks={Key:Value.to(device) for Key,Value in toks.items()}  # transfer the data on GPU\n","    encs=enc(input_ids=toks['input_ids'],attention_mask=toks['attention_mask'])\n","    print(encs.last_hidden_state.shape)\n","    # torch.Size([1, 20, 768]) for the 1st image\n","    # torch.Size([1, 14, 768]) for the 2nd image (only the length of sequence varies)\n","    # (batch size, sequence length, dimension of text embedding)\n","\n","    # Retrieving the image and transferring it to the vae\n","    photo_path = \"./coco_gsam_seg/\" + d[\"file_name\"][0:-4] + \"/src.jpg\"\n","    photo=Image.open(photo_path)\n","    photo=photo.resize((512, 512))\n","    photo_array=np.array(photo)\n","    photo_tensor=torch.tensor(photo_array, dtype=torch.float32).permute(2,0,1)/255.0\n","    photo_tensor=photo_tensor.unsqueeze(0).to(device)  # vae takes 4-dimension tensor as input so we must add a first dimension artificially\n","    print(photo_tensor.shape)\n","    # torch.Size([1, 3, 512, 512])\n","\n","    # encode the photo with autoencoder kl to obtain latent\n","    lat=vae.encode(photo_tensor)\n","    lat_tens=lat.latent_dist.sample() # obtain the tensor associated with the latent\n","    print(lat_tens.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # add noise\n","    # here I used the code from https://github.com/mlpc-ucsd/TokenCompose\n","    noise = torch.randn_like(lat_tens)\n","    bsz = lat_tens.shape[0]\n","    max_timestep = sch.config.num_train_timesteps\n","    timesteps = torch.randint(0, max_timestep, (bsz,), device=lat_tens.device)\n","    timesteps = timesteps.long()\n","    noisy_latents = sch.add_noise(lat_tens, noise, timesteps)\n","    print(noisy_latents.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # use unet\n","    prediction=une(noisy_latents,timesteps,encs.last_hidden_state).sample\n","    print(prediction.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # Loss LDM\n","    loss_ldm=torch.nn.functional.mse_loss(prediction,lat_tens)\n","    print(f\"loss_ldm: {loss_ldm}\")\n","    # tensor(16.7185, device='cuda:0', grad_fn=<MseLossBackward0>) for the 1st image\n","    # tensor(19.2932, device='cuda:0', grad_fn=<MseLossBackward0>) fir the 2nd image\n","\n","    # Ltoken\n","    ltoken=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      pixelwise_product=photo_tensor_token*prediction  # normally we use the attention activations but for the moment I don't have them (we look at that later and use \"prediction\" instead for the moment)\n","      sum_pixelwise=pixelwise_product.sum()\n","      sum_prediction=prediction.sum()\n","      ratio=sum_pixelwise/sum_prediction\n","      result=(1-ratio)**2\n","      ltoken+=result\n","    print(f\"ltoken: {ltoken}\")\n","\n","    # Lpixel\n","    # disabled because not working properly\n","    '''\n","    lpixel=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      log_prediction=torch.log(prediction)  # log(prediction)\n","      log_one_minus_prediction=torch.log(1-prediction)  # log(1-prediction)\n","      a=image_tensor_token*log_prediction  # image_tensor_token[u]*log(prediction[u])\n","      b=(1-image_tensor_token)*log_one_minus_prediction  # (1-image_tensor_token[u])*log(1-prediction[u])\n","      final_result=(a+b).sum()\n","      lpixel+=final_result\n","    print(f\"lpixel: {lpixel}\")\n","    '''\n","\n","    loss=loss_ldm+ltoken\n","    print(loss)\n"]},{"cell_type":"markdown","source":["### GPU (small test with 2 dataset images and batch size=2)\n","Big difference with before: remember to add `padding=True` in the tokenizer"],"metadata":{"id":"hygyJSwqwb3F"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device=torch.device(\"cuda\")\n","  print(f\"using GPU\")\n","else:\n","  device=torch.device(\"cpu\")\n","  print(f\"using CPU\")\n","\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","# load components\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder'].to(device)\n","vae=pipe.components['vae'].to(device)\n","une=pipe.components['unet'].to(device)\n","sch=pipe.components['scheduler']\n","\n","# create lists\n","photos=[]\n","texts=[]\n","attn_lists=[]\n","\n","# collect data\n","for d in data:\n","    texts.append(d[\"text\"])\n","    attn_lists.append(d[\"attn_list\"])\n","    photo_path=\"./coco_gsam_seg/\" + d[\"file_name\"][0:-4] + \"/src.jpg\"\n","    photo=Image.open(photo_path)\n","    photo=photo.resize((512, 512))\n","    photo_array=np.array(photo)\n","    photo_tensor=torch.tensor(photo_array, dtype=torch.float32).permute(2,0,1)/255.0\n","    photos.append(photo_tensor)\n","\n","# convert lists to tensors\n","texts_batch=tok(text=texts, return_tensors='pt',padding=True) # wde must activate padding here\n","texts_batch={Key: Value.to(device) for Key,Value in texts_batch.items()}\n","photos_batch=torch.stack([photo.to(device) for photo in photos])\n","\n","# go through encoder\n","encs=enc(input_ids=texts_batch['input_ids'], attention_mask=texts_batch['attention_mask'])\n","\n","# go through vae\n","latents=vae.encode(photos_batch)\n","lat_tens=latents.latent_dist.sample() # recover specifically the tensor\n","\n","# add noise\n","# here I used the code from https://github.com/mlpc-ucsd/TokenCompose\n","noise = torch.randn_like(lat_tens)\n","bsz = lat_tens.shape[0]  # Taille du batch\n","max_timestep=sch.config.num_train_timesteps\n","timesteps = torch.randint(0, max_timestep, (bsz,), device=lat_tens.device)\n","timesteps = timesteps.long()\n","noisy_latents=sch.add_noise(lat_tens, noise, timesteps)\n","\n","# use unet\n","predictions=une(noisy_latents,timesteps,encs.last_hidden_state).sample\n","\n","# Loss LDM\n","loss_ldm=torch.nn.functional.mse_loss(predictions,lat_tens)\n","\n","# Ltoken and Lpixel\n","ltoken=0.0\n","for k, d in enumerate(data):\n","    for token in d[\"attn_list\"]:\n","        photo_path_token=\"./coco_gsam_\"+token[1]\n","        photo_token=Image.open(photo_path_token)\n","        photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","        photo_array_token=np.array(photo_token)\n","        photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","\n","        photo_tensor_token=photo_tensor_token.to(device)\n","        photo_tensor_token=photo_tensor_token.unsqueeze(0)\n","\n","        # pixekwise product\n","        pixelwise_product=photo_tensor_token*predictions[k:k+1]  # instead of using the cross attention map I use predictions for the moment because I don't know how to get the cross attention map\n","        sum_pixelwise=pixelwise_product.sum()\n","        sum_prediction=predictions[k:k+1].sum()\n","        ratio=sum_pixelwise/sum_prediction\n","        result=(1-ratio)**2\n","        ltoken+=result\n","\n","# total loss\n","loss=loss_ldm +ltoken\n","print(loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rS95XeFowBl2","executionInfo":{"status":"ok","timestamp":1736154062408,"user_tz":-60,"elapsed":402,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"3d1543ed-8d1c-4724-df0c-8267ddcb6d33"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["using GPU\n","tensor(32.2268, device='cuda:0', grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"markdown","source":["### Where is the cross attention map ?"],"metadata":{"id":"Zsq9QRFmx53g"}},{"cell_type":"code","source":["une.attn_processors"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPqIf1dd5fqY","executionInfo":{"status":"ok","timestamp":1736154062408,"user_tz":-60,"elapsed":4,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"8f7c2536-aff5-4ad7-f923-6cc2742bb145"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e86547c0>,\n"," 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e86541c0>,\n"," 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8657100>,\n"," 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e86563b0>,\n"," 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8654850>,\n"," 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8657c40>,\n"," 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e86545b0>,\n"," 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8656830>,\n"," 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8c63580>,\n"," 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8c62230>,\n"," 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8c63460>,\n"," 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8c62e30>,\n"," 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e92ef0>,\n"," 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e922f0>,\n"," 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e934f0>,\n"," 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e92a70>,\n"," 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e90a60>,\n"," 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e93820>,\n"," 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a724a0>,\n"," 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a71f90>,\n"," 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a718a0>,\n"," 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a73dc0>,\n"," 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a722c0>,\n"," 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a71120>,\n"," 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a72290>,\n"," 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a726b0>,\n"," 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a70a90>,\n"," 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a70790>,\n"," 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a71ea0>,\n"," 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8a72110>,\n"," 'mid_block.attentions.0.transformer_blocks.0.attn1.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e93fd0>,\n"," 'mid_block.attentions.0.transformer_blocks.0.attn2.processor': <diffusers.models.attention_processor.AttnProcessor2_0 at 0x7995e8e91e70>}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### Try recovering attention map with a hook\n","\n","Using the previous code GPU (small test with 2 dataset images and batch size=1)"],"metadata":{"id":"HviiRvDvzqjp"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  device=torch.device(\"cuda\")\n","  print(f\"using GPU\")\n","else:\n","  device=torch.device(\"cpu\")\n","  print(f\"using CPU\")\n","\n","# for the moment we are only considering the first two lines of the dataset\n","data = [\n","    {\n","        \"file_name\": \"000000000142.jpg\",\n","        \"text\": \"Plate with a piece of bread, dark chocolate spread, bananas and a carton of Silk.\",\n","        \"attn_list\": [\n","            [\"piece\", \"seg/000000000142/mask_000000000142_piece.png\"],\n","            [\"carton\", \"seg/000000000142/mask_000000000142_carton.png\"],\n","            [\"Plate\", \"seg/000000000142/mask_000000000142_Plate.png\"],\n","            [\"Silk\", \"seg/000000000142/mask_000000000142_Silk.png\"],\n","            [\"bananas\", \"seg/000000000142/mask_000000000142_bananas.png\"],\n","            [\"bread\", \"seg/000000000142/mask_000000000142_bread.png\"],\n","            [\"chocolate spread\", \"seg/000000000142/mask_000000000142_chocolate_spread.png\"]\n","        ]\n","    },\n","    {\n","        \"file_name\": \"000000000370.jpg\",\n","        \"text\": \"A little girl holds a piece of broccoli towards the camera.\",\n","        \"attn_list\": [\n","            [\"girl\", \"seg/000000000370/mask_000000000370_girl.png\"],\n","            [\"broccoli\", \"seg/000000000370/mask_000000000370_broccoli.png\"]\n","        ]\n","    }\n","]\n","\n","# load components\n","tok=pipe.components['tokenizer']\n","enc=pipe.components['text_encoder'].to(device)\n","vae=pipe.components['vae'].to(device)\n","une=pipe.components['unet'].to(device)\n","sch=pipe.components['scheduler']\n","\n","for d in data:\n","    # Retrieve the prompt and run it through the tokenizer (CPU) then text_encoder (GPU)\n","    toks=tok(text=d[\"text\"],return_tensors='pt') # if I don't fill in return_tensors, the elements of toks will be lists (but encs takes tensors as inputs) -> 'pt'=\"pytorch\"\n","    toks={Key:Value.to(device) for Key,Value in toks.items()}  # transfer the data on GPU\n","    encs=enc(input_ids=toks['input_ids'],attention_mask=toks['attention_mask'])\n","    print(encs.last_hidden_state.shape)\n","    # torch.Size([1, 20, 768]) for the 1st image\n","    # torch.Size([1, 14, 768]) for the 2nd image (only the length of sequence varies)\n","    # (batch size, sequence length, dimension of text embedding)\n","\n","    # Retrieving the image and transferring it to the vae\n","    photo_path = \"./coco_gsam_seg/\" + d[\"file_name\"][0:-4] + \"/src.jpg\"\n","    photo=Image.open(photo_path)\n","    photo=photo.resize((512, 512))\n","    photo_array=np.array(photo)\n","    photo_tensor=torch.tensor(photo_array, dtype=torch.float32).permute(2,0,1)/255.0\n","    photo_tensor=photo_tensor.unsqueeze(0).to(device)  # vae takes 4-dimension tensor as input so we must add a first dimension artificially\n","    print(photo_tensor.shape)\n","    # torch.Size([1, 3, 512, 512])\n","\n","    # encode the photo with autoencoder kl to obtain latent\n","    lat=vae.encode(photo_tensor)\n","    lat_tens=lat.latent_dist.sample() # obtain the tensor associated with the latent\n","    print(lat_tens.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # add noise\n","    # here I used the code from https://github.com/mlpc-ucsd/TokenCompose\n","    noise = torch.randn_like(lat_tens)\n","    bsz = lat_tens.shape[0]\n","    max_timestep = sch.config.num_train_timesteps\n","    timesteps = torch.randint(0, max_timestep, (bsz,), device=lat_tens.device)\n","    timesteps = timesteps.long()\n","    noisy_latents = sch.add_noise(lat_tens, noise, timesteps)\n","    print(noisy_latents.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    '''\n","    We add the hook just right here\n","    '''\n","    # prepare list containing activations\n","    activations=[]\n","\n","    # I add a function that I use with the hook\n","    def attention_activations(module, input, output):\n","          activations.append(output)\n","\n","    transformer_block=une.down_blocks[0].attentions[0].transformer_blocks[0]  # the block from which I want to get activations\n","    transformer_block.register_forward_hook(attention_activations)\n","\n","    # use unet\n","    prediction=une(noisy_latents,timesteps,encs.last_hidden_state).sample\n","    print(prediction.shape)\n","    # torch.Size([1, 4, 64, 64])\n","\n","    # Loss LDM\n","    loss_ldm=torch.nn.functional.mse_loss(prediction,lat_tens)\n","    print(f\"loss_ldm: {loss_ldm}\")\n","    # tensor(16.7185, device='cuda:0', grad_fn=<MseLossBackward0>) for the 1st image\n","    # tensor(19.2932, device='cuda:0', grad_fn=<MseLossBackward0>) fir the 2nd image\n","\n","    # Ltoken\n","    ltoken=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      pixelwise_product=photo_tensor_token*prediction  # normally we use the attention activations but for the moment I don't have them (we look at that later and use \"prediction\" instead for the moment)\n","      sum_pixelwise=pixelwise_product.sum()\n","      sum_prediction=prediction.sum()\n","      ratio=sum_pixelwise/sum_prediction\n","      result=(1-ratio)**2\n","      ltoken+=result\n","    print(f\"ltoken: {ltoken}\")\n","\n","    # Lpixel\n","    # disabled because not working properly\n","    '''\n","    lpixel=0\n","    for token in d[\"attn_list\"]:\n","      photo_path_token=\"./coco_gsam_\"+token[1] # obtain the path to the attention mask\n","      photo_token=Image.open(photo_path_token)\n","      photo_token=photo_token.resize((64, 64)) # convert to the size of the latent\n","      photo_array_token=np.array(photo_token)\n","      photo_tensor_token=torch.tensor(photo_array_token, dtype=torch.float32).permute(2,0,1)/255.0\n","      print(photo_tensor_token.shape)\n","      # torch.Size([4, 64, 64])\n","\n","      photo_tensor_token=photo_tensor_token.to(device)\n","      photo_tensor_token=photo_tensor_token.unsqueeze(0)  # add a dimension to go from [4, 64, 64] to [1, 4, 64, 64]\n","      log_prediction=torch.log(prediction)  # log(prediction)\n","      log_one_minus_prediction=torch.log(1-prediction)  # log(1-prediction)\n","      a=image_tensor_token*log_prediction  # image_tensor_token[u]*log(prediction[u])\n","      b=(1-image_tensor_token)*log_one_minus_prediction  # (1-image_tensor_token[u])*log(1-prediction[u])\n","      final_result=(a+b).sum()\n","      lpixel+=final_result\n","    print(f\"lpixel: {lpixel}\")\n","    '''\n","\n","    loss=loss_ldm+ltoken\n","    print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8E22Yl9W59rE","executionInfo":{"status":"ok","timestamp":1736154062730,"user_tz":-60,"elapsed":4,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"1c846a16-8f80-45a6-d914-becd53ccac2a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["using GPU\n","torch.Size([1, 20, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 16.361553192138672\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 9.68333625793457\n","tensor(26.0449, device='cuda:0', grad_fn=<AddBackward0>)\n","torch.Size([1, 14, 768])\n","torch.Size([1, 3, 512, 512])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","torch.Size([1, 4, 64, 64])\n","loss_ldm: 19.07889747619629\n","torch.Size([4, 64, 64])\n","torch.Size([4, 64, 64])\n","ltoken: 1.4476203918457031\n","tensor(20.5265, device='cuda:0', grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["print(activations[1].shape) # renvoie [1,4096,320]. Or 4096 correspond à 64x64, clairement le nombre de cases de lat_tens\n","# Mais à quoi correspond 320 ?"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNBddTws73i1","executionInfo":{"status":"ok","timestamp":1736154062730,"user_tz":-60,"elapsed":3,"user":{"displayName":"Maxime CORLAY","userId":"04756669765464238120"}},"outputId":"f8fefb24-789d-4243-e096-ffcf3ae8bce0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 4096, 320])\n"]}]},{"cell_type":"markdown","source":["Ce que j'ai fait jusque là (fr, see english below):\n","* étudier en détail le code sur Hugging Face de S.D.1.4\n","* lire le papier (-> il y a une erreur sur le schéma mais globalement bon papier)\n","* tenter d'exécuter TokenCompose (-> j'ai eu un problème de bibliothèque)\n","* recoder mon propre TokenCompose grâce à ma connaissance du papier et des modules d'Hugging Face. Je me suis dit que ce serait finalement le meilleur moyen de comprendre ce qui se passe dans TokenCompose et unet. Je n'ai pas bien compris comment récupérer la carte des activations (dans les attentions layers) car dimension [1,4096,320] (j'ai bien compris que 4096 c'est pour 64x64 car l'espace latent est 64x64 mais je n'ai pas compris le rapport entre 320 et les dimensions de l'embedding textuel.\n","\n","Mes enseignements sur TokenCompose:\n","* papier bien rédigé sauf petite erreur sur le schéma\n","* j'ai pris conscience que la vraie difficulté à laquelle ils ont dû faire face est de récupérer les attentions (car je n'ai pas réussi pour l'instant)... mais j'ai bien compris que théoriquement avec un `hook` ça doit être possible\n","\n","Enseignements généraux:\n","* j'ai appris à manipuler Hugging Face (la plateforme)\n","* J'ai réalisé qu'Hugging Face est faite pour que les modèles soient utilisés, mais pas forcément fine tunés. Par exemple, SD14 n'a pas de fonction forward. il n'a qu'une fonction __call__\n","* on peut facilement accéder au code des modèles sur Hugging Face mais difficile à lire car beaucoup de classes imbriquées les unes dans les autres"],"metadata":{"id":"xph2FrcRBKJJ"}},{"cell_type":"markdown","source":["What I have done so far:\n","* study the Hugging Face code from S.D.1.4 in detail\n","* read the paper (-> there's an error in the schema but overall a good paper)\n","* try to run TokenCompose (-> I had a library problem)\n","* recode my own TokenCompose using my knowledge of the paper and Hugging Face modules. I figured this would ultimately be the best way to understand what was going on in TokenCompose and unet. I didn't quite understand how to get the activation map (in the attentions layers) because dimension [1,4096,320] (I understood that 4096 is for 64x64 because the latent space is 64x64 but I didn't understand the relationship between 320 and the dimensions of the text embedding.\n","\n","What I learned about TokenCompose:\n","* well-written paper except for a small error in the diagram\n","* I've realised that the real difficulty they've had to face is getting the attentions back (because I haven't managed that yet)... but I've understood that theoretically with a `hook` it should be possible.\n","\n","General lessons:\n","\n","* I learnt how to handle Hugging Face (the platform)\n","* I realised that Hugging Face is designed for models to be used, but not necessarily fine-tuned. For example, SD14 has no forward function, only a __call__ function.\n","* You can easily access the template code on Hugging Face, but it's difficult to read because there are so many classes nested inside each other."],"metadata":{"id":"_4MxEIMP16jX"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"63ac5207aaf84ad09c4a46f59d82db0f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04f9d45d296a4777b13906a9d07602ab","IPY_MODEL_a5f2c49ec52e48a8afddffa5f2acd93d","IPY_MODEL_08e5a5039da941b391350c469cdf8df3"],"layout":"IPY_MODEL_a3c1798c707f4df086e8afdc47eaeaeb"}},"04f9d45d296a4777b13906a9d07602ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54536a2a79f74c8981b07114c710ce10","placeholder":"​","style":"IPY_MODEL_7f0c9207d4e9470292d0574b88df8ac7","value":"Loading pipeline components...: 100%"}},"a5f2c49ec52e48a8afddffa5f2acd93d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d5be3fb464d4f9c9e9343c1403a59a4","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e2d2bca2d2a49f7afa7133b3bd0291c","value":7}},"08e5a5039da941b391350c469cdf8df3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69790ca66b574a4f804b507ae5088169","placeholder":"​","style":"IPY_MODEL_8661fd0d7ac94b8498c4428694821c54","value":" 7/7 [00:01&lt;00:00,  4.98it/s]"}},"a3c1798c707f4df086e8afdc47eaeaeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54536a2a79f74c8981b07114c710ce10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f0c9207d4e9470292d0574b88df8ac7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d5be3fb464d4f9c9e9343c1403a59a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e2d2bca2d2a49f7afa7133b3bd0291c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"69790ca66b574a4f804b507ae5088169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8661fd0d7ac94b8498c4428694821c54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
